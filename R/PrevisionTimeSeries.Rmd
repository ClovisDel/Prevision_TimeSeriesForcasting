---
title: |
  
author: 
- Clovis Deletre
- Charles Vitry
date:
output:
  html_notebook:
    theme: cerulean
    number_sections: no
    toc: yes
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 55px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 38px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 28px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 35px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("Fonctions.R", local = knitr::knit_global())

#install for export in pdf file
#tinytex::install_tinytex()
```

<br> </br>

```{r include=FALSE}
if(!require(forecast)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(forecast)

if(!require(fpp2)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(fpp2)

if(!require(MLmetrics)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(MLmetrics)

if(!require(ggplot2)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(ggplot2)

if(!require(fpp2)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(fpp2)

if(!require(TSstudio)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(TSstudio)

if(!require(ggthemes)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(ggthemes)

if(!require(timetk)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(timetk)


```



```{r include=FALSE}
if(!require(keras)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(keras)
if(!require(tensorflow)) install.packages("tm", repos = "http://cran.us.r-project.org")
require(tensorflow)
library(keras)
library(tensorflow)
#install_keras()
#install_tensorflow(version = "nightly")

```

# Introduction

Nous souhaitons réalisé l'**étude d'une série temporelle** et faire des
prévisions sur celle-ci.

Cette série temporelle est le trafic mensuel d'une Compagnie aérienne de
janvier 2011 à août 2019.

Nos prévisions portent sur les 8 mois de l'année 2019

# Représentation graphique de la série.

## Import des données

Import de la base, on sélectionne la colonne des valeurs

```{r}
library(readr)
data <- read_delim("Trafic-voyageurs.csv", 
    delim = ";", locale = locale(encoding = "ISO-8859-1"))
```

```{r}
summary(data)
```

```{r}
data_value <- data[,2]
```

## Affichage

Création de la série chronologique :

```{r}
library(TSstudio)
data_ts <- ts(data_value, start=2011, frequency=12)
plot_1_TimeSeries(data_ts)


```

## Séparation jeu de données

```{r}
#revoir l affichage car ca prend pas en compte tt 2019
data_ts_train <- window(data_ts, start = c(2011, 1), end = c(2018,12))
data_ts_test <- window(data_ts, start= c(2019,1), end = c(2019,8))

names(data)[1] <- "ds"
names(data)[2] <- "y"
data_train <- data[1:96,]
data_test <- data[97:104,]


plot(data_ts, xlim=c(2011,2020))
lines(data_ts_test, col=3)
legend("topleft", lty = 1, col=c(1,3), legend=c("Série chronologique Train", "Série chronologique Test"))
```

-> strong trend -> patern qui se repete, saisonnalité ?

## Représentation de la saisonnalité

Analyse de la saisonnalité en superposant chaque année (par mois):

-> en supprimant la tendance on voit bien la saisonnalité =>
saisonnalité régulière

```{r}
ggseasonplot(data_ts)
data_ts_without_trend = diff(data_ts)
ggseasonplot(data_ts_without_trend)
```

## Représentation des décompositions possibles

DECOMPOSITION : additive / Multiplicative Ts = Trend + Seasonal + Random
/ Ts = Trend \* Seasonal \* Random

```{r}
decomposed_data <- decompose(data_ts_train, type="additive")
plot(decomposed_data$trend)
plot(decomposed_data$seasonal)
plot(decomposed_data$random)

boxplot(data_ts ~ cycle(data_ts))
```

-> on distingue des saisonnalités => faire régression ca n'a pas de sens
=> modèle de Buys Ballot

-> bonne repartition du bruit -> quelques outliers

```{r}
checkresiduals(remainder(decomposed_data))
```

On a tendances + saisonnalité

# Modèles espace-état

-   meanf : Average Method : prend la valeur moyenne de toute les
    observations pour toutes les prédictions,
-   naive : Naive Method : prend la dernière observation pour toutes les
    prédictions,
-   drift : Drift Method : prend la première et la dernière observations
    et trace une lignes entre les deux, on utilise la courbe pour les
    prédictions,
-   snaive : Seasonal Naive Forecast : Prend la dernière valeur de la
    saison précédente comme prédiction (ex : sept 2018 = sep 2019 +
    erreur)

```{r}
library(forecast)
mean <- meanf(data_ts_train, h=8)
naivem <- naive(data_ts_train, h=8)
driftm <- rwf(data_ts_train, h=8, drif=T)
snaivem <- snaive(data_ts_train, h=8)
```

```{r}
plot(mean, plot.conf = F, main="")
lines(naivem$mean, col=2, lty=1)
lines(driftm$mean, col=5, lty=1)
lines(snaivem$mean, col = 4, lty=1)
legend("topleft", lty=1, col=c(1,2,3,4), legend=c("Mean Method", "Naive Method", "Drif Method", "Seasonal Naive"))


#comparaison :
plot(snaivem, plot.conf = F, main="")
lines(data_ts_test, col = 6, lty=1, lwd=3)

plot(driftm, plot.conf = F, main="")
lines(data_ts_test, col = 6, lty=1, lwd=3)

```

On regarde : MAE : Mean Absolute Error : RMSE : Root Mean Squarred Error

:   MASE : Mean Absolute Scaled Error : MAPE : Mean Absolute Percentage
    Error :

res = pred - val MAE = sum(abs(res))/length(val) RSS = sum(res\^2) MSE =
RSS/length(val) RMSE = sqrt(MSE)

La plus populaire est la MAPE

MAPE(y_pred, y_true)

\$MAPE = (1/n) \* Σ(\|actual -- forecast\| / \|actu0al\|) \* 10

"a MAPE value of 6% means that the average difference between the
forecasted value and the actual value is 6%"

```{r}
print(summary(mean))
checkresiduals(mean)
accuracy(mean, data_ts_test)

```

```{r}
print(summary(naivem))
checkresiduals(naivem)
accuracy(naivem, data_ts_test)

```

```{r}
print(summary(driftm))
checkresiduals(driftm)
accuracy(driftm, data_ts_test)

```

```{r}
print(summary(snaivem))
checkresiduals(snaivem)
accuracy(snaivem, data_ts_test)

```

# Etude du Modèle de Buys-Ballot

## Modèle

<https://mpra.ub.uni-muenchen.de/77718/1/MPRA_paper_77718.pdf> page 175

L'approche de BUYS-BALLOT consiste à introduire des variables
indicatrices correspondant à chaque saison définit par le cycle
d'observation. Pour les données trimestrielles, on intègre 4 variables
indicatrices. Et pour les données mensuelles, on intègre 12 variables
indicatrices.

Le modèle doit alors être estimé (sans constante) avec ces variables
indicatrices.

## Prédiction des valeurs de 2019

Préparation des données.

```{r}
Annees=as.numeric(time(data_ts_train))
ts_DataFrame =data.frame(trafic=data_ts_train,X=as.numeric(Annees))
```

Création du modèle

```{r}
Regression <- lm(trafic~X,data = ts_DataFrame)
```

$Xt = Zt + St + \mu t$

La tendance Prédiction sur les données futurs.

```{r}
tendance=predict(Regression)

AnneeMoisNumericFutur=seq(max(Annees)+1/12,length=8,by=1/12)  #les 10 prochains mois

tendance2=predict(Regression, newdata=data.frame(X=AnneeMoisNumericFutur)) 
```

```{r}
ts_DataFrame$trafic_residual <- residuals(Regression)
```

Définissons le mois

```{r}
ts_DataFrame$mois <- round(ts_DataFrame$X - trunc(ts_DataFrame$X),digit=4)
```

Création du 2nd modèle avec les mois

```{r}
Regression2 =lm(trafic_residual~0+as.factor(mois),data=ts_DataFrame)
```

Prédiction de la saisonnalité

```{r}
prediction2 =predict(Regression2)
```

Prédiction sur les mois

```{r}
MoisNumeric= round(AnneeMoisNumericFutur - trunc(AnneeMoisNumericFutur
                     ),4)
Prediction3 =predict( Regression2, newdata= data.frame(mois=MoisNumeric))

```

Calculons une région de confiance avec l'erreur d'ajustement

```{r}
ResidusRegression2=residuals(Regression2)
hist(ResidusRegression2)
1.96*sqrt(var(ResidusRegression2))
```

## Auto corrélation de la série temporelle

L'autocorrélation de notre série temporelle correspond à la corrélation
entre une mesure du trafic $t$ et les mesures précédentes $t - k$ ou les
mesures suivantes $t + k$.

L'auto covariance d'une variable $Xt$ de moyenne $\mu$ et d'écart type
$\sigma$ à un décalage $k$ est donné par la formule

$\gamma_k= E((X_t-\mu)(X_{t+k}-\mu))$

On en déduit l'autocorrélation correspondante :

$\rho_k=\frac{\gamma_k}{\sigma^2}$

Affichons les autocorrélations de la séries grâce à un corrélogramme

```{r}
ACF_Sur_Valeurs_Predites <- acf(prediction2)
```

Il est normal que la série soit autocorrélé totalement à elle avec un
décalage nulle.

On observe une corrélation forte (0.87) avec un décalage (lag) de 12,
cela correspond bien à une saisonnalité annuelle.

```{r}
print(data.frame(ACF_Sur_Valeurs_Predites$lag,ACF_Sur_Valeurs_Predites$acf)[1:13,])
```

Recalculons la valeur d'auto-corrélation obtenu en appliquant la
formule.

Observons l'application de la formule, en choisissant un décalage de 12

```{r}
#Constantes
Nombre_Observations=96
decalage=12

#Estimations
moyenneMu=mean(prediction2)
sdSigma=sd(prediction2)


Serie1=prediction2[(decalage+1): 96   ]
Serie2=prediction2[   1 :(96-decalage)]

GammaDecalage12=mean((Serie1-moyenneMu)*(Serie2-moyenneMu))*((Nombre_Observations-decalage)/(Nombre_Observations))

RhoDecalage12=GammaDecalage12/(sdSigma^2)
RhoDecalage12
```



Le résultat obtenu est correct. L'auto corrélation avec un décalage de 12 est donc très forte.

De plus cette auto corrélation étant positive, cela indique une tendance croissante.



la deuxième plus forte corrélation est obsersé avec un décalage de 5,
observons cela graphiquement

```{r}
plot  ( 1:length(prediction2),   prediction2,type="l")
points((1:length(prediction2))-5,prediction2,type="l",col="red")
```

Cette corrélation est peu pertinente.

```{r}
print(data.frame(ACF_Sur_Valeurs_Predites$lag,ACF_Sur_Valeurs_Predites$acf)[1:13,])
```
















Après avoir étudier les auto-corrélations sur l'ensemble du modèle,
Observons les auto-corrélations sur les résidus du modèle de
Buys-Ballot.

-   Texte pour dire que les accidents ne doivent pas être corrélés \*

```{r}
plot(acf(ResidusRegression2))
```

Pour notre modèle, il n'y a aucune auto-corrélation significative.
(symbolisé par la ligne bleu)

## Comparaison des prédictions et des valeurs réelles

Affichage de la tendance

```{r warning=FALSE}
Buys_ballot_plot_tendance <- plot(data_ts,
                         main = "Application du modèle de Buys_Ballot",
                         xlab = "Années",
                         ylab = "Nombre de Voyageurs") 

#droite de tendance
lines(Annees,tendance,col="blue",lwd=2)  

#prédiction de la tendance futur
lines(AnneeMoisNumericFutur,tendance2,col="red")


```

Affichage du modèle de Buys Ballot

```{r}

Buys_ballot_plot <- plot(data_ts,
                         main = "Application du modèle de Buys_Ballot",
                         xlab = "Années",
                         ylab = "Nombre de Voyageurs") 



#prédiction du modèle de Buys ballot
lines(Annees,tendance+prediction2,col="blue",lwd=2)

#Interval de confiance
 polygon(c(AnneeMoisNumericFutur,rev(AnneeMoisNumericFutur)),
 c(tendance2+Prediction3-1.96*sqrt(var(ResidusRegression2)),
 rev(tendance2+Prediction3+1.96*sqrt(var(ResidusRegression2)))),
 col="cadetblue1",border=NA)
 
 #Prediction des valeurs
 lines(AnneeMoisNumericFutur,tendance2+Prediction3,col="blue",lwd=2)
 
 
 lines(data_ts_test,col="black",lwd=3)
```

Affichage de la prédiction sur les 8 mois de 2020

```{r}

Buys_ballot_plot <- plot(data_ts_test,
                         main = "Application du modèle de Buys_Ballot",
                         xlab = "Années",
                         ylab = "Nombre de Voyageurs") 



#prédiction du modèle de Buys ballot
lines(Annees,tendance+prediction2,col="blue",lwd=2)

#Interval de confiance
 polygon(c(AnneeMoisNumericFutur,rev(AnneeMoisNumericFutur)),
 c(tendance2+Prediction3-1.96*sqrt(var(ResidusRegression2)),
 rev(tendance2+Prediction3+1.96*sqrt(var(ResidusRegression2)))),
 col="cadetblue1",border=NA)
 
 #Prediction des valeurs
 lines(AnneeMoisNumericFutur,tendance2+Prediction3,col="blue",lwd=2)
 
 
 lines(data_ts_test,col="black",lwd=3)
```

Préparation DataFrame pour affichage ggplot

```{r}
DataAffichageGGplot = as.data.frame(data_ts)
DataAffichageGGplot$Annees = c(Annees, AnneeMoisNumericFutur)
DataAffichageGGplot$AnneesRound = round(DataAffichageGGplot$Annees)
DataAffichageGGplot$PredictionTendance = c(tendance ,tendance2)
DataAffichageGGplot$BuysBalotModele = c(tendance+prediction2,tendance2+Prediction3 )


```

Reproduisons les graphiques avec ggplot2 pour un résultat plus
professsionnel.

```{r warning=FALSE}
library(ggplot2)
library(ggthemes)

p <- ggplot(data =DataAffichageGGplot, aes(x = Annees) ) + 

  geom_line(aes(y = trafic ), size = 0.9, alpha = 0.7)+

  #geom_line(aes(y = PredictionTendance), size = 0.6, alpha = 0.85,linetype="twodash" )+
  
  geom_line(aes(y = BuysBalotModele), size = 1.2, alpha = 0.6, color = "blue")+
  labs(title = "Application du modèle de Buys_Ballot",
       x="Années",
         y= "Nombre de Voyageurs")+
theme_fivethirtyeight()+
  theme(axis.title = element_text(), text = element_text(family = "Rubik")) 

#sur l'année 2019
p2 <- ggplot(data =DataAffichageGGplot, aes(x = Annees) ) + 
  geom_line(aes(y = trafic ), size = 1.2, alpha = 0.7)+
  geom_line(aes(y = BuysBalotModele), size = 1.4, alpha = 0.6, color = "blue")+
theme_fivethirtyeight()+
   xlim (2019.0, 2019.583) +
  ylim (435000, 520000) 


#Ajout zoom sur 2019
p + 
  annotation_custom(ggplotGrob(p2), xmin = 2015, xmax = 2020, ymin = 50000, ymax = 280000) +
  geom_rect(aes(xmin = 2015, xmax = 2020, ymin = 50000, ymax = 280000), color='black', linetype='dashed', alpha=0) 



```

Nous avons réussi à ajuster une droite de régression. on remarque que la
prédiction semble bien correspondre à la réalité si on fait abstraction
du dernier mois où le nombre de voyageurs a bien plus chuté que la
prédiction du modèle de Buys-Balot.

Comparons avec un ajustement local réalisé par lissage moyennes mobiles.

## Comparaison avec les valeurs observées

# Lissage moyenne mobile

## Définition

Mettre belle formule en latex ici

## Choix Moyenne mobiles

## Conservation & Annulation






# Lissage exponentielle

## Lissage simple

```{r}
fcst_se <- ses(data_ts_train, h = 8)
print(summary(fcst_se))
checkresiduals(fcst_se)
```

```{r}
plot(fcst_se)
lines(data_ts_test, col="red")


df_se = as.data.frame(fcst_se)
predict_value_se <- df_se$`Point Forecast`
MAPE(predict_value_se, data_ts_test)*100
```

## Optimisation du modèle

Fit Exponential Smoothing model -> trouve le meilleur lissage expo

```{r}
fit_ets <- ets(data_ts_train) 
print(summary(fit_ets))
checkresiduals(fit_ets)


```

```{r}
fcst_ets <- forecast(fit_ets, h=8)
plot(fcst_ets)
lines(data_ts_test, col="red")


df_ets = as.data.frame(fcst_ets)
predict_value_ets = df_ets$`Point Forecast`
MAPE(predict_value_ets, data_ts_test)*100

```

## Modèle ARMA


A FAIRE


## Modèle ARIMA / SAMIRA Automatique

ARIMA : AutoRegressive Integrated Moving Average

Le modèle ARIMA est une combinaison du modèle ARMA combiné à une différentiation (le Integrated)

Différentiation = rétirer les tendances 
  -> tendance linéaire : une différenciation 
  -> tendance quadradique : deux différenciations 

Le modèle SARIMA est une combinaison du modèle ARIMA qui prend en compte la composante saisonniaire. 

auto.arima prend en compte les saisonnalités, comme on peut le voir dans le modèle selectionné :
(0,1,1)(0,1,1)[12]


```{r}
# retourne les meilleurs paramètres 
# d=1 enleve la tendance
# D=1 enleve la saisonnalité 
# => avoir des données stationnaires
# trace : voir les résultats
fit_arima <- auto.arima(data_ts_train, d=1, D=1, stepwise = FALSE, approximation = FALSE, trace=TRUE)
print(summary(fit_arima))
checkresiduals(fit_arima)
```

```{r}
fcst_arima <- forecast(fit_arima, h=8)
plot(fcst_arima)
lines(data_ts_test, col='red')


df_arima = as.data.frame(fcst_arima)
predict_value_arima = df_arima$`Point Forecast`
MAPE(predict_value_arima, data_ts_test)*100
```



## PROPHET

Préparation données
```{r}
library(zoo)
data_train$ds <- as.Date( as.yearmon(time(data_ts_train)))
```



A COMMENTER ET A FAIRE FONCTIONNER SURTT (changement de la forme des dates?)
```{r}
library(prophet)
```


## MODELE POSSIBLE :

LSTM
DeepAR
N-BEATS
Temporal Fusion Transformer



## LSTM 

```{r}

scale_factors <- c(mean(data$y), sd(data$y))
scaled_train <- data %>%
    dplyr::select(y) %>%
    dplyr::mutate(y = (y - scale_factors[1]) / scale_factors[2])
scaled_train



prediction <- 12
lag <- prediction
```
On veut prendre l'année précedente pour apprendre > lag de 12,
en réalité ca fait 12 - 1 pour avoir à chaque prédiction basée sur 12 valeurs

puis en transforme en array 3D car le modèle LSTM prendre un tensor de format 3D [samples, timesteps, features]
  samples : nbr d'observation par batchs
  timesteps : lag
  features : nbr de valeur predites 


```{r}
scaled_train <- as.matrix(scaled_train)
 
# we lag the data 11 times and arrange that into columns
x_train_data <- t(sapply(
    1:(length(scaled_train) - lag - prediction + 1),
    function(x) scaled_train[x:(x + lag - 1), 1]
  ))
 
# now we transform it into 3D form
x_train_arr <- array(
    data = as.numeric(unlist(x_train_data)),
    dim = c(
        nrow(x_train_data),
        lag,
        1
    )
)

#(x_train_data)
#length(x_train_arr)
#head(x_train_arr)
```


```{r}
y_train_data <- t(sapply(
    (1 + lag):(length(scaled_train) - prediction + 1),
    function(x) scaled_train[x:(x + prediction - 1)]
))

y_train_arr <- array(
    data = as.numeric(unlist(y_train_data)),
    dim = c(
        nrow(y_train_data),
        prediction,
        1
    )
)

#head(y_train_data)
#head(y_train_arr)
```


```{r}
x_test <- data$y[(nrow(scaled_train) - prediction + 1):nrow(scaled_train)]

x_test_scaled <- (x_test - scale_factors[1]) / scale_factors[2]

x_pred_arr <- array(
    data = x_test_scaled,
    dim = c(
        1,
        lag,
        1
    )
)

```


```{r}
lstm_model <- keras_model_sequential()

lstm_model %>%
  layer_lstm(units = 50, # size of the layer
       batch_input_shape = c(1, 12, 1), # batch size, timesteps, features
       return_sequences = TRUE,
       stateful = TRUE) %>%
  # fraction of the units to drop for the linear transformation of the inputs
  layer_dropout(rate = 0.5) %>%
  layer_lstm(units = 50,
        return_sequences = TRUE,
        stateful = TRUE) %>%
  layer_dropout(rate = 0.5) %>%
  time_distributed(keras::layer_dense(units = 1))

lstm_model %>%
    compile(loss = 'mae', optimizer = 'adam', metrics = 'accuracy')

summary(lstm_model)


```

```{r}
lstm_model %>% fit(
    x = x_train_arr,
    y = y_train_arr,
    batch_size = 1,
    epochs = 20,
    verbose = 1,
    shuffle = FALSE
)
```

```{r}
lstm_forecast <- lstm_model %>%
    predict(x_pred_arr, batch_size = 1) %>%
    .[, , 1]
 
# rescale en format basique
lstm_forecast <- lstm_forecast * scale_factors[2] + scale_factors[1]
lstm_forecast
```


 X résultats / prédictions par input donc > transforme pour une seule prédiciton
```{r}
fitted <- predict(lstm_model, x_train_arr, batch_size = 1) %>%
     .[, , 1]

if (dim(fitted)[2] > 1) {
    fit <- c(fitted[, 1], fitted[dim(fitted)[1], 2:dim(fitted)[2]])
} else {
    fit <- fitted[, 1]
}

# rescale final de nos données
fitted <- fit * scale_factors[2] + scale_factors[1]
fitted
fitted <- c(rep(NA, lag), fitted)
fitted
length(fitted)

```



```{r}
lstm_forecast <- ts(lstm_forecast,
    start = c(2019, 1),
    end = c(2019, 12),
    frequency = 12
)

lstm_forecast_display <- window(lstm_forecast, start= c(2019,1), end = c(2019,8))

input_ts <- ts(data$y, 
    start = c(2011, 1), 
    end = c(2018, 12), 
    frequency = 12)


lstm_forecast_display
data_ts_test

plot(input_ts, xlim=c(2011,2020))
#lines(data_ts_test)
lines(lstm_forecast_display, col=3)



```

